# -*- coding: utf-8 -*-
"""Code for SouthPark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oyk64tsxLiP2O32WYhUxOpn_orpDMtqg

# Importing Libraries
"""

import nltk
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt

"""# Importing Dataset and reducing the dimensions and no. of classes"""

'''from google.colab import files
uploaded=files.upload()
import io
df = pd.read_csv(io.BytesIO(uploaded['All-seasons.csv']))'''
df=pd.read_csv('All-seasons.csv')

df.describe()

df.head()

from collections import Counter
c= Counter(df.Character)
print(len(c))
for key,val in c.items():
  print("{0:20}{1:20}".format(key,val))

"""This dataset contains dialogues and speakers from season 9 and 10 of the animated series south park.
The season and the episode are irrelevant to our task, i.e., to predict the speaker from the dialogue.
"""

classes=['Cartman','Stan','Kyle','Butters','Randy','Mr. Garrison','Kenny','Chef']
data=df.loc[df['Character'].isin(classes)]
data=data.iloc[:,2:4]
print(data.head(10))
shape=data.shape
print(shape)

c=Counter(data.Character)
plt.bar(c.keys(),c.values())

"""# Creating Corpus

function for obtaining the part of speech for lemmatisation
"""

#nltk.download('popular')
from nltk import wordnet
def get_wordnet_pos(tag):
 
    if tag.startswith('J'):
        return "a"
    elif tag.startswith('V'):
        return "v"
    elif tag.startswith('N'):
        return "n"
    elif tag.startswith('R'):
        return "r"
    else:
        return "n"

"""cleaning the texts and creating the corpus"""

#nltk.download('popular')
corpus=[]
from nltk.stem import WordNetLemmatizer
lemmatiser=WordNetLemmatizer()
for dialog in data.Line:
    dialog=nltk.word_tokenize(dialog)
    for i in range(0,len(dialog)):
        dialog[i]= re.sub('\'s','is',dialog[i])
        dialog[i]=re.sub('\'m','am',dialog[i])
        dialog[i]=re.sub('n\'t','not',dialog[i])
        dialog[i]=re.sub('\'ve','have',dialog[i])
        dialog[i]=re.sub('[^a-zA-Z]',' ',dialog[i])
        dialog[i]=dialog[i].lower()
    pos=nltk.pos_tag(dialog)
    dialog=[]
    for word,tag in pos:
        if (word is not ' '):
            l=lemmatiser.lemmatize(word,get_wordnet_pos(tag))
            dialog.append(l)
    dialog=' '.join(dialog)
    corpus.append(dialog)

for w in corpus:
  print("\n",w)

"""Encoding Target Variable"""

y=data['Character']
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y)
le.classes_

"""# Creating the Bag of Words model"""

from sklearn.feature_extraction.text import TfidfVectorizer
tv = TfidfVectorizer(ngram_range=(3,3),max_features=15000)
x = tv.fit_transform(corpus).toarray()

"""# Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import StratifiedKFold
kfold=StratifiedKFold(n_splits=5, shuffle=True, random_state=1)

"""# training the model"""

from sklearn.naive_bayes import MultinomialNB
rf=MultinomialNB()
from sklearn.model_selection import cross_val_predict, cross_val_score
score=np.mean(cross_val_score(rf,x,y,cv=kfold))

print(score)
y_pred=cross_val_predict(rf,x,y,cv=kfold)