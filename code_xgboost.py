# -*- coding: utf-8 -*-
"""Code_xgboost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16wfp6wqQK9TRszVsEu_j2eevF_TRtP7E

# Importing Libraries
"""

import nltk
import pandas as pd
import numpy as np
import re
import xgboost as xgb

"""# Importing Dataset and reducing the dimensions and no. of classes"""

from google.colab import files
uploaded=files.upload()
import io
df = pd.read_csv(io.BytesIO(uploaded['All-seasons.csv']))
#df=pd.read_csv('All-seasons.csv')

"""This dataset contains dialogues and speakers from season 9 and 10 of the animated series south park.
The season and the episode are irrelevant to our task, i.e., to predict the speaker from the dialogue.
"""

classes=['Cartman','Stan','Kyle','Butters','Randy','Mr. Garrison','Kenny']
data=df.loc[df['Character'].isin(classes)]
data=data.iloc[:,2:4]
print(data.head(10))
shape=data.shape
print(shape)

"""# Creating Corpus

function for obtaining the part of speech for lemmatisation
"""

#nltk.download('popular')
from nltk import wordnet
def get_wordnet_pos(tag):
 
    if tag.startswith('J'):
        return "a"
    elif tag.startswith('V'):
        return "v"
    elif tag.startswith('N'):
        return "n"
    elif tag.startswith('R'):
        return "r"
    else:
        return "n"

"""cleaning the texts and creating the corpus"""

nltk.download('popular')
corpus=[]
from nltk.stem import WordNetLemmatizer
lemmatiser=WordNetLemmatizer()
for dialog in data.Line:
    dialog=nltk.word_tokenize(dialog)
    for i in range(0,len(dialog)):
        dialog[i]= re.sub('\'s','is',dialog[i])
        dialog[i]=re.sub('\'m','am',dialog[i])
        dialog[i]=re.sub('n\'t','not',dialog[i])
        dialog[i]=re.sub('\'ve','have',dialog[i])
        dialog[i]=re.sub('[^a-zA-Z]',' ',dialog[i])
        dialog[i]=dialog[i].lower()
    pos=nltk.pos_tag(dialog)
    dialog=[]
    for word,tag in pos:
        if (word is not ' '):
            l=lemmatiser.lemmatize(word,get_wordnet_pos(tag))
            dialog.append(l)
    dialog=' '.join(dialog)
    corpus.append(dialog)

"""Encoding Target Variable"""

y=data['Character']
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y)
le.classes_

"""# Creating the Bag of Words model"""

from sklearn.feature_extraction.text import TfidfVectorizer
tv = TfidfVectorizer(ngram_range=(3,3),max_features=500)
x = tv.fit_transform(corpus).toarray()

"""# Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)

"""# training the model"""

from sklearn.datasets import dump_svmlight_file
dump_svmlight_file(X_train, y_train, 'dtrain.svm', zero_based=True)
dump_svmlight_file(X_test, y_test, 'dtest.svm', zero_based=True)
dtrain_svm = xgb.DMatrix('dtrain.svm')
dtest_svm = xgb.DMatrix('dtest.svm')

param = {
    'max_depth': 5,  # the maximum depth of each tree
    'eta': 0.1,  # the training step for each iteration
    'silent': 1,  # logging mode - quiet
    'objective': 'multi:softprob',  # error evaluation for multiclass training
    'num_class': 8}  # the number of classes that exist in this datset
num_round = 100  # the number of training iterations
bst = xgb.train(param, dtrain_svm, num_round)
preds = bst.predict(dtest_svm)
best_preds = np.asarray([np.argmax(line) for line in preds])

from sklearn.metrics import accuracy_score, confusion_matrix
print(accuracy_score(y_test, best_preds))
print(confusion_matrix(y_test, best_preds))